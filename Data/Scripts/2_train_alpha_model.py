"""
File 2: Train and Evaluate Both Alpha and Alpha-Beta Models in Parallel

This script reads the multi-tab dataset generated by the updated data generator
and trains two separate models in parallel:
1. A single-output model predicting only 'alpha'.
2. A multi-output model predicting both 'alpha' and 'beta'.

Each training process includes its own hyperparameter tuning and generates a
full set of performance reports, plots, and a final serialized model.
"""
import os
import pandas as pd
import lightgbm as lgb
from sklearn.metrics import r2_score, mean_absolute_error
from sklearn.multioutput import MultiOutputRegressor
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import optuna
import concurrent.futures

# --- Configuration ---
SCRIPT_DIR = Path(__file__).parent
DATA_DIR = SCRIPT_DIR / "ML_model"
DATASET_PATH = DATA_DIR / "vrp_tour_dataset.xlsx" # <-- Read from Excel
PLOTS_DIR = DATA_DIR
N_TUNING_TRIALS = 100

def train_alpha_only_model(df: pd.DataFrame):
    """Tunes, trains, and evaluates a single-output alpha prediction model."""
    print("\n--- [START] Training Alpha-Only Model ---")
    MODEL_SAVE_PATH = DATA_DIR / "alpha_predictor_model.joblib"
    
    target_col = 'alpha'
    feature_cols = [c for c in df.columns if c not in ['instance', 'partition', 'alpha', 'true_tsp_cost', 'mst_length']]
    X_train, y_train = df[df['partition']=='train'][feature_cols], df[df['partition']=='train'][target_col]
    X_val, y_val = df[df['partition']=='validation'][feature_cols], df[df['partition']=='validation'][target_col]
    X_test, y_test = df[df['partition']=='test'][feature_cols], df[df['partition']=='test'][target_col]

    def objective(trial):
        params = {'objective': 'regression_l1', 'metric': 'mae', 'n_estimators': 1000, 'verbosity': -1, 'n_jobs': 1, 'seed': 42,
                  'learning_rate': trial.suggest_float('learning_rate', 1e-3, 5e-2, log=True),
                  'num_leaves': trial.suggest_int('num_leaves', 20, 300)}
        model = lgb.LGBMRegressor(**params)
        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(100, verbose=False)])
        return mean_absolute_error(y_val, model.predict(X_val))

    study = optuna.create_study(direction='minimize')
    study.optimize(objective, n_trials=N_TUNING_TRIALS)
    
    best_params = study.best_params
    best_params.update({'objective': 'regression_l1', 'metric': 'mae', 'n_estimators': 2000, 'random_state': 42, 'n_jobs': 1})
    final_model = lgb.LGBMRegressor(**best_params)
    final_model.fit(pd.concat([X_train, X_val]), pd.concat([y_train, y_val]))
    
    y_pred = final_model.predict(X_test)
    r2, mae = r2_score(y_test, y_pred), mean_absolute_error(y_test, y_pred)
    print("\n--- Alpha-Only Model Performance (Test Set) ---")
    print(f"R-squared (R²): {r2:.4f}, MAE: {mae:.4f}")
    
    joblib.dump(final_model, MODEL_SAVE_PATH)
    print(f"Alpha-Only Model saved to {MODEL_SAVE_PATH}")
    print("--- [DONE] Training Alpha-Only Model ---")
    return True

def train_alpha_beta_model(df: pd.DataFrame):
    """Tunes, trains, and evaluates a multi-output alpha-beta model."""
    print("\n--- [START] Training Alpha-Beta Model ---")
    MODEL_SAVE_PATH = DATA_DIR / "alpha_beta_predictor_model.joblib"
    
    target_cols = ['target_alpha', 'target_beta']
    feature_cols = [c for c in df.columns if c not in ['instance', 'partition', 'alpha', 'true_tsp_cost', 'mst_length'] + target_cols]
    X_train, y_train = df[df['partition']=='train'][feature_cols], df[df['partition']=='train'][target_cols]
    X_val, y_val = df[df['partition']=='validation'][feature_cols], df[df['partition']=='validation'][target_cols]
    X_test, y_test = df[df['partition']=='test'][feature_cols], df[df['partition']=='test'][target_cols]

    def objective(trial):
        params = {'objective': 'regression_l1', 'metric': 'mae', 'n_estimators': 1000, 'verbosity': -1, 'n_jobs': 1, 'seed': 42,
                  'learning_rate': trial.suggest_float('learning_rate', 1e-3, 5e-2, log=True),
                  'num_leaves': trial.suggest_int('num_leaves', 20, 300)}
        model = MultiOutputRegressor(lgb.LGBMRegressor(**params), n_jobs=1)
        model.fit(X_train, y_train)
        preds = model.predict(X_val)
        return mean_absolute_error(y_val['target_alpha'], preds[:, 0])

    study = optuna.create_study(direction='minimize')
    study.optimize(objective, n_trials=N_TUNING_TRIALS)

    best_params = study.best_params
    best_params.update({'objective': 'regression_l1', 'metric': 'mae', 'n_estimators': 2000, 'random_state': 42, 'n_jobs': 1})
    final_model = MultiOutputRegressor(lgb.LGBMRegressor(**best_params), n_jobs=1)
    final_model.fit(pd.concat([X_train, X_val]), pd.concat([y_train, y_val]))
    
    y_pred = final_model.predict(X_test)
    r2_alpha, mae_alpha = r2_score(y_test['target_alpha'], y_pred[:, 0]), mean_absolute_error(y_test['target_alpha'], y_pred[:, 0])
    r2_beta, mae_beta = r2_score(y_test['target_beta'], y_pred[:, 1]), mean_absolute_error(y_test['target_beta'], y_pred[:, 1])

    print("\n--- Alpha-Beta Model Performance (Test Set) ---")
    print(f"Alpha Prediction -> R²: {r2_alpha:.4f}, MAE: {mae_alpha:.4f}")
    print(f"Beta Prediction  -> R²: {r2_beta:.4f}, MAE: {mae_beta:.4f}")

    joblib.dump(final_model, MODEL_SAVE_PATH)
    print(f"Alpha-Beta Model saved to {MODEL_SAVE_PATH}")
    print("--- [DONE] Training Alpha-Beta Model ---")
    return True

def main():
    """Main function to orchestrate parallel model training."""
    print("--- Parallel Model Training Orchestrator ---")
    if not DATASET_PATH.exists():
        print(f"FATAL: Dataset not found at {DATASET_PATH}. Please run the data generator first.")
        return

    print(f"Loading data from both tabs of {DATASET_PATH}...")
    try:
        all_sheets = pd.read_excel(DATASET_PATH, sheet_name=None)
        df_alpha = all_sheets['alpha_only_data']
        df_alpha_beta = all_sheets['alpha_beta_data']
    except Exception as e:
        print(f"FATAL: Could not read Excel file. Error: {e}")
        return

    # Use a ProcessPoolExecutor to run both training functions in parallel
    with concurrent.futures.ProcessPoolExecutor(max_workers=2) as executor:
        print("\nSubmitting both model training jobs to run in parallel...")
        future_alpha = executor.submit(train_alpha_only_model, df_alpha)
        future_alpha_beta = executor.submit(train_alpha_beta_model, df_alpha_beta)

        # Wait for results and report completion
        for future in concurrent.futures.as_completed([future_alpha, future_alpha_beta]):
            try:
                result = future.result()
                if not result:
                    print("A training job failed. Check logs above.")
            except Exception as exc:
                print(f'A training job generated an exception: {exc}')

    print("\n\nAll training processes complete.")

if __name__ == "__main__":
    main()