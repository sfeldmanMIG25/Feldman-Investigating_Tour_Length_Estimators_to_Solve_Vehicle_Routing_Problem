#!/usr/bin/env python3
# run_ortools_benchmark.py
import os
import sys
import time
from pathlib import Path
import numpy as np
import pandas as pd
import concurrent.futures
from tqdm import tqdm

from ortools.constraint_solver import routing_enums_pb2, pywrapcp

# --- Configuration ---

# Set a 6-hour timeout for each VRP instance solver.
HEURISTIC_TIMEOUT = 6 * 3600

# Define paths relative to the script's location.
SCRIPT_DIR = Path(__file__).parent
sys.path.append(str(SCRIPT_DIR))

# Import the VRP parsing utility from your existing file.
from vrp_utils import parse_vrp

# Define data and output directories.
DATA_DIR = SCRIPT_DIR.parent
INSTANCE_DIR_MEDIUM = DATA_DIR / "instances_medium"
INSTANCE_DIR_LARGE = DATA_DIR / "instances_large"

# The script will create these directories to store the generated solution files.
OUTPUT_DIR_MEDIUM = DATA_DIR / "solutions_medium_ortools"
OUTPUT_DIR_LARGE = DATA_DIR / "solutions_large_ortools"

# This control file dictates which instances will be run.
# It should be generated by your 'run_XMLLarge_Sample.py' script.
CONTROL_CSV_PATH = SCRIPT_DIR / "Batch_Run_Results_tabu2_large_GART" / 'benchmark_instances_xml_large.csv'


def solve_instance_with_ortools(instance_path: Path, output_path: Path, time_limit: int):
    """
    Solves a single CVRP instance using Google OR-Tools and writes the solution.

    Args:
        instance_path: The full path to the .vrp instance file.
        output_path: The full path where the .sol output file will be saved.
        time_limit: The maximum time in seconds allowed for the solver.
    """
    try:
        # 1. Parse instance data using the provided utility function.
        capacity, depot_id, coords, demands = parse_vrp(str(instance_path))
        
        # OR-Tools requires node indices from 0 to N-1. We create a mapping
        # from the original node IDs to this 0-based index.
        node_ids = sorted(coords.keys())
        id_to_index = {nid: i for i, nid in enumerate(node_ids)}
        depot_index = id_to_index[depot_id]
        num_nodes = len(node_ids)

        # 2. Create the data model for OR-Tools.
        data = {}
        data['demands'] = [demands.get(nid, 0) for nid in node_ids]
        data['vehicle_capacities'] = [capacity] * num_nodes # Assume enough vehicles
        data['num_vehicles'] = num_nodes
        data['depot'] = depot_index

        # 3. Create the distance matrix. OR-Tools works with integers,
        # so we calculate Euclidean distance and round to the nearest integer.
        dist_matrix = np.zeros((num_nodes, num_nodes), dtype=int)
        coords_array = np.array([coords[nid] for nid in node_ids])
        for i in range(num_nodes):
            for j in range(i, num_nodes):
                dist = np.linalg.norm(coords_array[i] - coords_array[j])
                dist_matrix[i, j] = dist_matrix[j, i] = int(dist + 0.5)

        # 4. Set up the OR-Tools Routing Model.
        manager = pywrapcp.RoutingIndexManager(num_nodes, data['num_vehicles'], data['depot'])
        routing = pywrapcp.RoutingModel(manager)

        def distance_callback(from_index, to_index):
            from_node = manager.IndexToNode(from_index)
            to_node = manager.IndexToNode(to_index)
            return dist_matrix[from_node][to_node]

        transit_callback_index = routing.RegisterTransitCallback(distance_callback)
        routing.SetArcCostEvaluatorOfAllVehicles(transit_callback_index)

        def demand_callback(from_index):
            from_node = manager.IndexToNode(from_index)
            return data['demands'][from_node]

        demand_callback_index = routing.RegisterUnaryTransitCallback(demand_callback)
        routing.AddDimensionWithVehicleCapacity(
            demand_callback_index,
            0,  # null capacity slack
            data['vehicle_capacities'],
            True, # start cumul to zero
            'Capacity'
        )

        # 5. Configure the search parameters for high-quality solutions.
        search_parameters = pywrapcp.DefaultRoutingSearchParameters()
        
        # Start with a good initial solution.
        search_parameters.first_solution_strategy = (
            routing_enums_pb2.FirstSolutionStrategy.PATH_CHEAPEST_ARC
        )
        # Use an advanced metaheuristic to improve upon the initial solution.
        # GUIDED_LOCAL_SEARCH is a powerful and commonly used option.
        search_parameters.local_search_metaheuristic = (
            routing_enums_pb2.LocalSearchMetaheuristic.GUIDED_LOCAL_SEARCH
        )
        search_parameters.time_limit.FromSeconds(time_limit)
        search_parameters.log_search = True

        # 6. Solve the problem and record the time.
        start_time = time.time()
        solution = routing.SolveWithParameters(search_parameters)
        end_time = time.time()

        # 7. Write the solution to the output file.
        if solution:
            cost = solution.ObjectiveValue()
            with open(output_path, 'w') as f:
                f.write(f"Cost: {cost}\n")
                f.write(f"Time: {end_time - start_time:.4f}\n")
        else:
             with open(output_path, 'w') as f:
                f.write("Cost: -1\n") # Indicate failure
                f.write(f"Time: {end_time - start_time:.4f}\n")

        return f"Successfully processed {instance_path.name}"
    except Exception as e:
        return f"ERROR processing {instance_path.name}: {e}"


def process_instance_worker(instance_filename: str):
    """
    A worker function for the process pool that determines the correct paths
    and calls the main OR-Tools solver function.
    """
    # Determine if the instance is medium or large to select the correct directory.
    if "10000" in instance_filename:
        instance_dir = INSTANCE_DIR_LARGE
        output_dir = OUTPUT_DIR_LARGE
    else:
        instance_dir = INSTANCE_DIR_MEDIUM
        output_dir = OUTPUT_DIR_MEDIUM
    
    instance_path = instance_dir / instance_filename
    output_path = output_dir / instance_filename.replace('.vrp', '.sol')
    
    return solve_instance_with_ortools(instance_path, output_path, HEURISTIC_TIMEOUT)


def aggregate_results():
    """
    Parses all generated .sol files and aggregates the results into a
    single Excel file for analysis.
    """
    print("\n--- Aggregating OR-Tools Results ---")
    if not CONTROL_CSV_PATH.exists():
        print(f"FATAL: Control file not found at '{CONTROL_CSV_PATH}'. Cannot aggregate.")
        return

    instances_df = pd.read_csv(CONTROL_CSV_PATH)
    results = []

    for instance_file in tqdm(instances_df['instance_filename'], desc="Aggregating"):
        basename = instance_file.replace('.vrp', '')
        
        if "10000" in basename:
            sol_path = OUTPUT_DIR_LARGE / f"{basename}.sol"
        else:
            sol_path = OUTPUT_DIR_MEDIUM / f"{basename}.sol"

        if sol_path.exists():
            cost, solve_time = np.nan, np.nan
            with open(sol_path, 'r') as f:
                for line in f:
                    if line.lower().startswith("cost:"):
                        cost = float(line.split(":")[1].strip())
                    elif line.lower().startswith("time:"):
                        solve_time = float(line.split(":")[1].strip())
            results.append({
                'instance': basename,
                'solver': 'OR-Tools',
                'cost': cost,
                'time_seconds': solve_time
            })
    
    if not results:
        print("No solution files found to aggregate.")
        return

    df_results = pd.DataFrame(results)
    excel_path = SCRIPT_DIR / "ortools_summary_results.xlsx"
    df_results.to_excel(excel_path, index=False, float_format="%.2f")
    print(f"\nâœ… Aggregated results saved to '{excel_path}'")


def main():
    """
    Main function to orchestrate the benchmark run. It creates directories,
    runs solvers in parallel, and then aggregates the results.
    """
    # Ensure output directories exist before starting.
    OUTPUT_DIR_MEDIUM.mkdir(exist_ok=True)
    OUTPUT_DIR_LARGE.mkdir(exist_ok=True)

    if not CONTROL_CSV_PATH.exists():
        print(f"FATAL: Control CSV file not found at '{CONTROL_CSV_PATH}'.")
        print("Please run 'run_XMLLarge_Sample.py' first to generate the file.")
        sys.exit(1)

    instances_to_run = pd.read_csv(CONTROL_CSV_PATH)['instance_filename'].tolist()
    num_workers = os.cpu_count()

    print(f"Starting OR-Tools batch run for {len(instances_to_run)} instances.")
    print(f"Timeout per instance: {HEURISTIC_TIMEOUT / 3600:.1f} hours.")
    print(f"Using {num_workers} parallel processes.")

    # Use a process pool to run instances in parallel for maximum speed.
    with concurrent.futures.ProcessPoolExecutor(max_workers=num_workers) as executor:
        results = list(tqdm(
            executor.map(process_instance_worker, instances_to_run),
            total=len(instances_to_run),
            desc="Solving Instances"
        ))

    # Print any errors that occurred during the run.
    for res in results:
        if "ERROR" in res:
            print(res)

    # After all solvers have finished, collect the results into a single file.
    aggregate_results()
    print("\n--- OR-Tools Benchmark Run Complete ---")


if __name__ == "__main__":
    main()